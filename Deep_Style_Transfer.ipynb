{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep_Style_Transfer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thangtulaloinoi/deep-photo-transfer/blob/main/Deep_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chuVvdHj3qgW"
      },
      "source": [
        "#Deep Style Transfer\n",
        "\n",
        "- Neural style transfer using a pretrained [VGG19] model as the feature extractor.                                  \n",
        "- Photorealism regularization\n",
        "- Style Loss With Semantic Segmentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxUicSPUOP6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OLbOWhPCO"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "from imageio import mimsave\n",
        "from IPython.display import display as display_fn\n",
        "from IPython.display import Image, clear_output\n",
        "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "import scipy.ndimage\n",
        "import scipy.sparse\n",
        "import scipy.sparse.linalg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4Yt8nArTeR"
      },
      "source": [
        "## Utilities Image Preprocessing Function\n",
        "\n",
        "Use utility functions below to help in loading, visualizing, and preprocessing the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  '''converts a tensor to an image'''\n",
        "  tensor_shape = tf.shape(tensor)\n",
        "  number_elem_shape = tf.shape(tensor_shape)\n",
        "  if number_elem_shape > 3:\n",
        "    assert tensor_shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return tf.keras.preprocessing.image.array_to_img(tensor) \n",
        "\n",
        "\n",
        "def load_img(path_to_img):\n",
        "  '''loads an image as a tensor and scales it to 512 pixels'''\n",
        "  max_dim = 512\n",
        "  image = tf.io.read_file(path_to_img)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  shape = tf.shape(image)[:-1]\n",
        "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "  image = image[tf.newaxis, :]\n",
        "  image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def load_images(content_path, style_path):\n",
        "  '''loads the content and path images as tensors'''\n",
        "  content_image = load_img(\"{}\".format(content_path))\n",
        "  style_image = load_img(\"{}\".format(style_path))\n",
        "\n",
        "  return content_image, style_image\n",
        "\n",
        "\n",
        "def imshow(image, title=None):\n",
        "  '''displays an image with a corresponding title'''\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "    \n",
        "    \n",
        "def show_images_with_objects(images, titles=[]):\n",
        "  '''displays a row of images with corresponding titles'''\n",
        "  if len(images) != len(titles):\n",
        "    return\n",
        "\n",
        "  plt.figure(figsize=(20, 12))\n",
        "  for idx, (image, title) in enumerate(zip(images, titles)):\n",
        "    plt.subplot(1, len(images), idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    imshow(image, title)\n",
        "\n",
        "\n",
        "def display_gif(gif_path):\n",
        "  '''displays the generated images as an animated gif'''\n",
        "  with open(gif_path,'rb') as f:\n",
        "    display_fn(Image(data=f.read(), format='png'))\n",
        "\n",
        "\n",
        "def create_gif(gif_path, images):\n",
        "  '''creates animation of generated images'''\n",
        "  mimsave(gif_path, images, fps=1)\n",
        "  \n",
        "  return gif_path\n",
        "\n",
        "\n",
        "def clip_image_values(image, min_value=0.0, max_value=255.0):\n",
        "  '''clips the image pixel values by the given min and max'''\n",
        "  return tf.clip_by_value(image, clip_value_min=min_value, clip_value_max=max_value)\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "  '''centers the pixel values of a given image to use with VGG-19'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "\n",
        "  return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U9It5Ii2Oof"
      },
      "source": [
        "## Load Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF8S_z1CKSd5"
      },
      "source": [
        "# set default images\n",
        "content_path = '/content/input02.jpg'\n",
        "style_path = '/content/style02.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTQJfhYc7db4"
      },
      "source": [
        "# display the content and style image\n",
        "content_image, style_image = load_images(content_path, style_path)\n",
        "show_images_with_objects([content_image, style_image], \n",
        "                         titles=[f'content image: {content_path}',\n",
        "                                 f'style image: {style_path}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3i3RRrJiOX"
      },
      "source": [
        "## Build the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2V1h9U--C7v"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1xNii3cDPob5cX8QpXPu3S3ps8s9O5X15\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48cNdd0N3qgg"
      },
      "source": [
        "Using the VGG-19 model as the feature extractor. You will feed in the style and content image and depending on the computed losses, a new image will be generated which has elements of both the content and style image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JEI_tdT3qgg"
      },
      "source": [
        "# clear session to make layer naming consistent when re-running this cell\n",
        "K.clear_session()\n",
        "\n",
        "# download the vgg19 model and inspect the layers\n",
        "tmp_vgg = tf.keras.applications.vgg19.VGG19()\n",
        "tmp_vgg.summary()\n",
        "\n",
        "# delete temporary variable\n",
        "del tmp_vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-tASys0eJv"
      },
      "source": [
        "Choose intermediate layers from the network to extract the style and content of the image:\n",
        "\n",
        "- For the style layers, you will use the first layer of each convolutional block.\n",
        "\n",
        "- For the content layer, you will use the second convolutional layer of the last convolutional block (just one layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArfX_6iA0WAX"
      },
      "source": [
        "# style layers of interest\n",
        "style_layers = ['block1_conv1', \n",
        "                'block2_conv1', \n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1'] \n",
        "\n",
        "# choose the content layer and put in a list\n",
        "content_layers = ['block4_conv2'] \n",
        "\n",
        "# combine the two lists (put the style layers before the content layers)\n",
        "output_layers = style_layers + content_layers \n",
        "\n",
        "# declare auxiliary variables holding the number of style and content layers\n",
        "NUM_CONTENT_LAYERS = len(content_layers)\n",
        "NUM_STYLE_LAYERS = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKw5AaWB3qgg"
      },
      "source": [
        "Define your model to take the same input as the standard VGG-19 model, and output just the selected content and style layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfec6MuMAbPx"
      },
      "source": [
        "def vgg_model(layer_names):\n",
        "  \"\"\" Creates a vgg model that outputs the style and content layer activations.\n",
        "  \n",
        "  Args:\n",
        "    layer_names: a list of strings, representing the names of the desired content and style layers\n",
        "    \n",
        "  Returns:\n",
        "    A model that takes the regular vgg19 input and outputs just the content and style layers.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  # load the the pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "  # freeze the weights of the model's layers (make them not trainable)\n",
        "  vgg.trainable = False\n",
        "  \n",
        "  # create a list of layer objects that are specified by layer_names\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  # create the model that outputs content and style layers only\n",
        "  model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEROBesg3qgh"
      },
      "source": [
        "Create an instance of the model using the function that you just defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RJLgwAn3qgi"
      },
      "source": [
        "# clear session to make layer naming consistent if re-running the cell\n",
        "K.clear_session()\n",
        "\n",
        "# create a vgg-19 model\n",
        "vgg = vgg_model(output_layers)\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yfSrdvFNe3b"
      },
      "source": [
        "## Define the loss functions\n",
        "\n",
        "Next, you will define functions to compute the losses required for generating the new image. These would be the:\n",
        "\n",
        "* Content Loss\n",
        "* Style Loss\n",
        "* Photorealism Regularization \n",
        "* Total Loss (combination of style and content loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRzTkttk3qgi"
      },
      "source": [
        "### Calculate content loss\n",
        "\n",
        "The content loss will be the sum of the squared error between the features and targets, then multiplied by a scaling factor (0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et8M1lOgKL8o"
      },
      "source": [
        "def get_content_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  \n",
        "  Args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "  \n",
        "  Returns:\n",
        "    content loss (scalar)\n",
        "  \"\"\"\n",
        "  # get the sum of the squared error multiplied by a scaling factor\n",
        "  content_loss = 0.5 * tf.reduce_sum(tf.square(features - targets))\n",
        "    \n",
        "  return content_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaIvZf5wWn_"
      },
      "source": [
        "### Calculate style loss\n",
        "\n",
        "The style loss is the average of the squared differences between the features and targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv8hZU0oKIm_"
      },
      "source": [
        "def get_style_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  \n",
        "  Args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "\n",
        "  Returns:\n",
        "    style loss (scalar)\n",
        "  \"\"\"\n",
        "  # get the average of the squared errors\n",
        "  style_loss = tf.reduce_mean(tf.square(features - targets))\n",
        "    \n",
        "  return style_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTCuv2663qgi"
      },
      "source": [
        "### Calculate the gram matrix\n",
        "\n",
        "Use `tf.linalg.einsum` to calculate the gram matrix for an input tensor.\n",
        "- In addition, calculate the scaling factor `num_locations` and divide the gram matrix calculation by `num_locations`.\n",
        "\n",
        "$$ \\text{num locations} = height \\times width $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAy1iGPdoEpZ"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  \"\"\" Calculates the gram matrix and divides by the number of locations\n",
        "  Args:\n",
        "    input_tensor: tensor of shape (batch, height, width, channels)\n",
        "    \n",
        "  Returns:\n",
        "    scaled_gram: gram matrix divided by the number of locations\n",
        "  \"\"\"\n",
        "\n",
        "  # calculate the gram matrix of the input tensor\n",
        "  gram = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor) \n",
        "\n",
        "  # get the height and width of the input tensor\n",
        "  input_shape = tf.shape(input_tensor) \n",
        "  height = input_shape[1] \n",
        "  width = input_shape[2] \n",
        "\n",
        "  # get the number of locations (height times width), and cast it as a tf.float32\n",
        "  num_locations = tf.cast(height * width, tf.float32)\n",
        "\n",
        "  # scale the gram matrix by dividing by the number of locations\n",
        "  scaled_gram = gram / num_locations\n",
        "    \n",
        "  return scaled_gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqA0ffs13qgk"
      },
      "source": [
        "- For each style layer, calculate the gram matrix.  Store these results in a list and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTK5qzG_MKh"
      },
      "source": [
        "def get_style_image_features(image):  \n",
        "  \"\"\" Get the style image features\n",
        "  \n",
        "  Args:\n",
        "    image: an input image\n",
        "    \n",
        "  Returns:\n",
        "    gram_style_features: the style features as gram matrices\n",
        "  \"\"\"\n",
        "  # preprocess the image using the given preprocessing function\n",
        "  preprocessed_style_image = preprocess_image(image) \n",
        "\n",
        "  # get the outputs from the custom vgg model that you created using vgg_model()\n",
        "  outputs = vgg(preprocessed_style_image) \n",
        "\n",
        "  # Get just the style feature layers (exclude the content layer)\n",
        "  style_outputs = outputs[:NUM_STYLE_LAYERS] \n",
        "\n",
        "  # for each style layer, calculate the gram matrix for that layer and store these results in a list\n",
        "  gram_style_features = [gram_matrix(style_layer) for style_layer in style_outputs] \n",
        "\n",
        "  return gram_style_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwUzBs023qgk"
      },
      "source": [
        "### Get content image features\n",
        "\n",
        "Now you will get the content features of an image.\n",
        "- You can follow a similar process as you did with `get_style_image_features()`.\n",
        "- You will not calculate the gram matrix of these features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7rq02U9_a6L"
      },
      "source": [
        "def get_content_image_features(image):\n",
        "  \"\"\" Get the content image features\n",
        "  \n",
        "  Args:\n",
        "    image: an input image\n",
        "    \n",
        "  Returns:\n",
        "    content_outputs: the content features of the image\n",
        "  \"\"\"\n",
        "  # preprocess the image\n",
        "  preprocessed_content_image = preprocess_image(image)\n",
        "    \n",
        "  # get the outputs from the vgg model\n",
        "  outputs = vgg(preprocessed_content_image) \n",
        "\n",
        "  # get the content layers of the outputs\n",
        "  content_outputs = outputs[NUM_STYLE_LAYERS:]\n",
        "\n",
        "  # return the content layer outputs of the content image\n",
        "  return content_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB9ZCNbq3qgk"
      },
      "source": [
        "### Calculate the total loss\n",
        "\n",
        "The total loss is given by $L_{total} = \\beta L_{style} + \\alpha L_{content} + \\lambda L_{m}$, where $\\beta$ and $\\alpha$ are weights we will give to the content and style features to generate the new image. See how it is implemented in the function below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q20XhIHnotQA"
      },
      "source": [
        "def get_style_content_loss(image, content_image, style_targets, style_outputs, content_targets, \n",
        "                           content_outputs, style_weight = 100, content_weight = 1, lambda_weight = 10 ** 4):\n",
        "  \"\"\" Combine the style and content loss\n",
        "  \n",
        "  Args:\n",
        "    style_targets: style features of the style image\n",
        "    style_outputs: style features of the generated image\n",
        "    content_targets: content features of the content image\n",
        "    content_outputs: content features of the generated image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    lambda_weight: weight given to relularization\n",
        "\n",
        "  Returns:\n",
        "    total_loss: the combined style and content loss\n",
        "\n",
        "  \"\"\"\n",
        "    \n",
        "  # sum of the style losses\n",
        "  style_loss = tf.add_n([ get_style_loss(style_output, style_target)\n",
        "                           for style_output, style_target in zip(style_outputs, style_targets)])\n",
        "  \n",
        "  # Sum up the content losses\n",
        "  content_loss = tf.add_n([get_content_loss(content_output, content_target)\n",
        "                           for content_output, content_target in zip(content_outputs, content_targets)])\n",
        "\n",
        "  # scale the style loss by multiplying by the style weight and dividing by the number of style layers\n",
        "  style_loss = style_loss * style_weight / NUM_STYLE_LAYERS \n",
        "\n",
        "  # scale the content loss by multiplying by the content weight and dividing by the number of content layers\n",
        "  content_loss = content_loss * content_weight / NUM_CONTENT_LAYERS \n",
        "\n",
        "  #Regularization\n",
        "  regularization = calculate_photorealism_regularization(image, content_image)\n",
        "                        \n",
        "    \n",
        "  # sum up the style and content losses\n",
        "  total_loss = style_loss + content_loss + lambda_weight*regularization\n",
        "\n",
        "  return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDoqox9sDoNP"
      },
      "source": [
        "###Style Loss By Marks Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H3N7AA_Ddkq"
      },
      "source": [
        "def calculate_layer_style_loss(style_layer, transfer_layer, content_masks, style_masks):\n",
        "    # scale masks to current layer\n",
        "    content_size = tf.TensorShape(transfer_layer.shape[1:3])\n",
        "    style_size = tf.TensorShape(style_layer.shape[1:3])\n",
        "\n",
        "    def resize_masks(masks, size):\n",
        "        return [tf.image.resize_bilinear(mask, size) for mask in masks]\n",
        "\n",
        "    style_masks = resize_masks(style_masks, style_size)\n",
        "    content_masks = resize_masks(content_masks, content_size)\n",
        "\n",
        "    feature_map_count = np.float32(transfer_layer.shape[3].value)\n",
        "    feature_map_size = np.float32(transfer_layer.shape[1].value) * np.float32(transfer_layer.shape[2].value)\n",
        "\n",
        "    means_per_channel = []\n",
        "    for content_mask, style_mask in zip(content_masks, style_masks):\n",
        "        transfer_gram_matrix = calculate_gram_matrix(transfer_layer, content_mask)\n",
        "        style_gram_matrix = calculate_gram_matrix(style_layer, style_mask)\n",
        "\n",
        "        mean = tf.reduce_mean(tf.squared_difference(style_gram_matrix, transfer_gram_matrix))\n",
        "        means_per_channel.append(mean / (2 * tf.square(feature_map_count) * tf.square(feature_map_size)))\n",
        "\n",
        "    style_loss = tf.reduce_sum(means_per_channel)\n",
        "\n",
        "    return style_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvTdfjv_D59L"
      },
      "source": [
        "### Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_26Ap6jD8K6"
      },
      "source": [
        "def calculate_photorealism_regularization(output, content_image):\n",
        "    # normalize content image and out for matting and regularization computation\n",
        "    content_image = content_image / 255.0\n",
        "    output = tf.Variable(output)\n",
        "    output = output / 255.0\n",
        "\n",
        "    # compute matting laplacian\n",
        "    matting = compute_laplacian(content_image[0, ...])\n",
        "\n",
        "    # compute photorealism regularization loss\n",
        "    regularization_channels = []\n",
        "    for output_channel in tf.unstack(output, axis=-1):\n",
        "        channel_vector = tf.reshape(tf.transpose(output_channel), shape=[-1])\n",
        "        matmul_right =  tf.sparse.sparse_dense_matmul(matting, tf.expand_dims(channel_vector, -1))\n",
        "        matmul_left = tf.matmul(tf.expand_dims(channel_vector, 0), matmul_right)\n",
        "        regularization_channels.append(matmul_left)\n",
        "\n",
        "    regularization = tf.reduce_sum(regularization_channels)\n",
        "    return regularization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yA80iSPEJL-"
      },
      "source": [
        "### Laplacian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHHKvX-xEOji"
      },
      "source": [
        "def _rolling_block(A, block=(3, 3)):\n",
        "    \"\"\"Applies sliding window to given matrix.\"\"\"\n",
        "    shape = (A.shape[0] - block[0] + 1, A.shape[1] - block[1] + 1) + block\n",
        "    strides = (A.strides[0], A.strides[1]) + A.strides\n",
        "    return as_strided(A, shape=shape, strides=strides)\n",
        "\n",
        "def compute_laplacian(img, mask=None, eps=10**(-7), win_rad=1):\n",
        "    \"\"\"Computes Matting Laplacian for a given image.\n",
        "    Args:\n",
        "        img: 3-dim numpy matrix with input image\n",
        "        mask: mask of pixels for which Laplacian will be computed.\n",
        "            If not set Laplacian will be computed for all pixels.\n",
        "        eps: regularization parameter controlling alpha smoothness\n",
        "            from Eq. 12 of the original paper. Defaults to 1e-7.\n",
        "        win_rad: radius of window used to build Matting Laplacian (i.e.\n",
        "            radius of omega_k in Eq. 12).\n",
        "    Returns: sparse matrix holding Matting Laplacian.\n",
        "    \"\"\"\n",
        "\n",
        "    win_size = (win_rad * 2 + 1) ** 2\n",
        "    h, w, d = img.shape\n",
        "    # Number of window centre indices in h, w axes\n",
        "    c_h, c_w = h - 2 * win_rad, w - 2 * win_rad\n",
        "    win_diam = win_rad * 2 + 1\n",
        "\n",
        "    indsM = np.arange(h * w).reshape((h, w))\n",
        "    ravelImg = img.reshape(h * w, d)\n",
        "    win_inds = _rolling_block(indsM, block=(win_diam, win_diam))\n",
        "\n",
        "    win_inds = win_inds.reshape(c_h, c_w, win_size)\n",
        "    if mask is not None:\n",
        "        mask = cv2.dilate(\n",
        "            mask.astype(np.uint8),\n",
        "            np.ones((win_diam, win_diam), np.uint8)\n",
        "        ).astype(np.bool)\n",
        "        win_mask = np.sum(mask.ravel()[win_inds], axis=2)\n",
        "        win_inds = win_inds[win_mask > 0, :]\n",
        "    else:\n",
        "        win_inds = win_inds.reshape(-1, win_size)\n",
        "\n",
        "    \n",
        "    winI = ravelImg[win_inds]\n",
        "\n",
        "    win_mu = np.mean(winI, axis=1, keepdims=True)\n",
        "    win_var = np.einsum('...ji,...jk ->...ik', winI, winI) / win_size - np.einsum('...ji,...jk ->...ik', win_mu, win_mu)\n",
        "\n",
        "    inv = np.linalg.inv(win_var + (eps/win_size)*np.eye(3))\n",
        "\n",
        "    X = np.einsum('...ij,...jk->...ik', winI - win_mu, inv)\n",
        "    vals = np.eye(win_size) - (1.0/win_size)*(1 + np.einsum('...ij,...kj->...ik', X, winI - win_mu))\n",
        "\n",
        "    nz_indsCol = np.tile(win_inds, win_size).ravel()\n",
        "    nz_indsRow = np.repeat(win_inds, win_size).ravel()\n",
        "    nz_indsVal = vals.ravel()\n",
        "    L = scipy.sparse.coo_matrix((nz_indsVal, (nz_indsRow, nz_indsCol)), shape=(h*w, h*w))\n",
        "    \n",
        "    sum_a = L.sum(axis=1).T.tolist()[0]\n",
        "    L = (scipy.sparse.diags([sum_a], [0], shape=(h*w, h*w)) - L) \\\n",
        "        .tocoo()\n",
        "\n",
        "    indices = np.mat([L.row, L.col]).transpose()\n",
        "    laplacian_tf = tf.cast(tf.SparseTensor(indices, L.data, L.shape), dtype=tf.float32)\n",
        "    return laplacian_tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JViezXeqPyrY"
      },
      "source": [
        "## Generate the Stylized Image\n",
        "\n",
        "You will now define helper functions to generate the new image given the total loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrBIWelC3qgl"
      },
      "source": [
        "### Calculate gradients\n",
        "\n",
        "First is the function to calculate the gradients. The values here will be used to update the generated image to have more of the style and content features. \n",
        "\n",
        "*Note: If you are still in Lesson 1, please disregard the `var_weight` parameter. That will be defined and discussed in Lesson 2.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp2g2tI58RI0"
      },
      "source": [
        "def calculate_gradients(image, content_image, style_targets, content_targets, \n",
        "                        style_weight, content_weight, lambda_weight):\n",
        "  \"\"\" Calculate the gradients of the loss with respect to the generated image\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "  \n",
        "  Returns:\n",
        "    gradients: gradients of the loss with respect to the input image\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "      \n",
        "    # get the style image features\n",
        "    style_features = get_style_image_features(image) \n",
        "      \n",
        "    # get the content image features\n",
        "    content_features = get_content_image_features(image) \n",
        "      \n",
        "    # get the style and content loss\n",
        "    loss = get_style_content_loss(image, content_image, style_targets, style_features, content_targets, \n",
        "                                  content_features, style_weight, content_weight, lambda_weight) \n",
        "\n",
        "  # calculate gradients of loss with respect to the image\n",
        "  gradients = tape.gradient(loss, image) \n",
        "\n",
        "  return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-crYKGz3qgl"
      },
      "source": [
        "### Update the image with the style\n",
        "\n",
        "Similar to model training, you will use an optimizer to update the original image from the computed gradients. Since we're dealing with images, we want to clip the values to the range we expect. That would be `[0, 255]` in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-MPRxuGp-5A"
      },
      "source": [
        "def update_image_with_style(image, content_image, style_targets, content_targets, style_weight, \n",
        "                            content_weight, lambda_weight, optimizer):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "    optimizer: optimizer for updating the input image\n",
        "  \"\"\"\n",
        "\n",
        "  # calculate gradients using the function that you just defined.\n",
        "  gradients = calculate_gradients(image, content_image, style_targets, content_targets, \n",
        "                                  style_weight, content_weight, lambda_weight) \n",
        "\n",
        "  # apply the gradients to the given image\n",
        "  optimizer.apply_gradients([(gradients, image)]) \n",
        "\n",
        "  # clip the image using the utility clip_image_values() function\n",
        "  image.assign(clip_image_values(image, min_value=0.0, max_value=255.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTOpNNw2Wp2"
      },
      "source": [
        "## Style Transfer\n",
        "\n",
        "You can now define the main loop. This will use the previous functions you just defined to generate the stylized content image. It does so incrementally based on the computed gradients and the number of epochs. Visualizing the output at each epoch is also useful so you can quickly see if the style transfer is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Btr_j9M1gu"
      },
      "source": [
        "def fit_style_transfer(style_image, content_image, style_weight, content_weight, lambda_weight,\n",
        "                        optimizer='adam', epochs=1, steps_per_epoch=1):\n",
        "  \"\"\" Performs neural style transfer.\n",
        "  Args:\n",
        "    style_image: image to get style features from\n",
        "    content_image: image to stylize \n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "    optimizer: optimizer for updating the input image\n",
        "    epochs: number of epochs\n",
        "    steps_per_epoch = steps per epoch\n",
        "  \n",
        "  Returns:\n",
        "    generated_image: generated image at final epoch\n",
        "    images: collection of generated images per epoch  \n",
        "  \"\"\"\n",
        "\n",
        "  images = []\n",
        "  step = 0\n",
        "\n",
        "  # get the style image features \n",
        "  style_targets = get_style_image_features(style_image)\n",
        "    \n",
        "  # get the content image features\n",
        "  content_targets = get_content_image_features(content_image)\n",
        "\n",
        "  # initialize the generated image for updates\n",
        "  generated_image = tf.cast(content_image, dtype=tf.float32)\n",
        "  generated_image = tf.Variable(generated_image) \n",
        "  \n",
        "  # collect the image updates starting from the content image\n",
        "  images.append(content_image)\n",
        "  \n",
        "  # incrementally update the content image with the style features\n",
        "  for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "      step += 1\n",
        "    \n",
        "      # Update the image with the style using the function that you defined\n",
        "      update_image_with_style(generated_image, content_image, style_targets, content_targets, \n",
        "                              style_weight, content_weight, lambda_weight, optimizer) \n",
        "    \n",
        "      print(\".\", end='')\n",
        "\n",
        "      if (m + 1) % 10 == 0:\n",
        "        images.append(generated_image)\n",
        "    \n",
        "    # display the current stylized image\n",
        "    clear_output(wait=True)\n",
        "    display_image = tensor_to_image(generated_image)\n",
        "    display_fn(display_image)\n",
        "\n",
        "    # append to the image collection for visualization later\n",
        "    images.append(generated_image)\n",
        "    print(\"Train step: {}\".format(step))\n",
        "  \n",
        "  # convert to uint8 (expected dtype for images with pixels in the range [0,255])\n",
        "  generated_image = tf.cast(generated_image, dtype=tf.uint8)\n",
        "\n",
        "  return generated_image, images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dPRr8BqexB"
      },
      "source": [
        "style_weight =  100\n",
        "content_weight = 1\n",
        "lambda_weight = 10 ** 2\n",
        "\n",
        "adam = tf.optimizers.Adam(\n",
        "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=20.0, decay_steps=100, decay_rate=0.50\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "stylized_image_reg, display_images_reg = fit_style_transfer(style_image=style_image, content_image=content_image, \n",
        "                                                    style_weight=style_weight, content_weight=content_weight, lambda_weight=lambda_weight, optimizer=adam, epochs=10, steps_per_epoch=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lla8IAunRviU"
      },
      "source": [
        "# Display Frequency Variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "\n",
        "stylized_image_reg_x_deltas, stylized_image_reg_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(stylized_image_reg, dtype=tf.float32))\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_reg_x_deltas, stylized_image_reg_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ZUGUXO1v8A"
      },
      "source": [
        "Notice that the variations are generally smoother with the additional parameter. Here are the stylized images again with and without regularization for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POtMRtWBAz21"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image_reg], titles=['Style Image', 'Content Image', 'Stylized Image with Regularization'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTm-iSzTmX8J"
      },
      "source": [
        "show_images_with_objects([stylized_image_reg], titles=['Stylized Image with Regularization'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}